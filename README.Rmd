---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

```{r, include=FALSE}
set.seed(0)
```

# Reinforcement Learning

[![Build Status](https://travis-ci.org/nproellochs/ReinforcementLearning.svg?branch=master)](https://travis-ci.org/nproellochs/ReinforcementLearning)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/ReinforcementLEarning)](https://cran.r-project.org/package=ReinforcementLearning)

**ReinforcementLearning** performs model-free reinforcement learning in R. This implementation enables the learning of
    an optimal policy based on sample sequences consisting of states, actions and rewards. In 
    addition, it supplies multiple predefined reinforcement learning algorithms, such as experience 
    replay.

## Overview

The most important functions of **ReinforcementLearning** are:

- Learning an optimal policy from a fixed set of a priori known transition samples
- Predefined learning rules and action selection modes
- A highly customizable framework for model-free reinforcement learning tasks

## Installation

Using the **devtools** package, you can easily install the latest development version of **ReinforcementLearning** with

```{r,eval=FALSE}
install.packages("devtools")

# Recommended option: download and install latest version from "GitHub"
devtools::install_github("nproellochs/ReinforcementLearning")
```

Notes: 

* A CRAN version has not yet been released.

## Usage

This section shows the basic functionality of how to perform reinforcement learning. First, load the corresponding package **ReinforcementLearning**.

```{r, message=FALSE}
library(ReinforcementLearning)
```

The following example shows how to learn a reinforcement learning agent using input data in the form of sample sequences 
consisting of states, actions and rewards. The result of the learning process is a state-action table and an optimal 
policy that defines the best possible action in each state. 

```{r, message=FALSE}
# Generate sample experience in the form of state transition tuples
data <- sampleGridSequence(N = 1000)
head(data)

# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1)

# Perform reinforcement learning
model <- ReinforcementLearning(data, s = "State", a = "Action", r = "Reward", 
                               s_new = "NextState", control = control)

# Print result
print(model)
```

## License

**ReinforcementLearning** is released under the [MIT License](https://opensource.org/licenses/MIT)

Copyright (c) 2017 Nicolas PrÃ¶llochs & Stefan Feuerriegel
